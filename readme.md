1. Clone CodeLlama-7b-Instruct-hf and CodeLlama-70b-Instruct-hf into a directory above or directly in this repo. 
2. Use fp16_to_int4.py to convert into a singular int4 quantized model. 
3. Run quantize.py to sample. 

